---
title: "Boosted trees"
subtitle: "Series 2.3.1 - AdaBoost, ?Histogram-based gradient boosting and Scikit-mol"
author: Jennifer HY Lin
date: '2024-4-24'
draft: true
categories: 
    - Machine learning projects
    - Tree models
    - Pandas
    - Scikit-learn
    - RDKit
    - ChEMBL database
    - Python
jupyter: python3
format: html
bibliography: references.bib
---

##### **Some introductions**

Adaptive Boost or AdaBoost has originated from Robert E. Schapire in 1990 [@schapire1990], [@mlbook2022], and was further introduced in 1996 by Robert Schapire and Yoav Freund at a conference which also led to a publication [@freund1997].

As quoted from [scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#adaboost), an AdaBoost algorithm is doing this:

> ...fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data.

Note: weak learners = an ensemble of very simple base classifiers e.g. decision tree stumps [@mlbook2022]

During the process of running the algorithm, increased weights are given to the incorrectly predicted samples at each iteration, and less weights are given to the correctly predicted ones. This then forces the AdaBoost models to focus more on the less accurately predicted samples with the aim to improve ensemble performance. The predictions from these iterations are combined to produce a final prediction via a weighted majority vote style, which is a well-known signature of tree models. Overall, AdaBoost algorithm can be used for classification or regression problems. The main difference between bagging and boosting is that boosting only uses random subsets of training samples drawn from the training dataset without any replacements [@mlbook2022]. One caveat to note is that AdaBoost tend to overfit training data (high variance).

Parameters to tune:

* n_estimators - number of weak learners

* learning_rate - contributions of weak learners in the final combination

* max_depth - depth of trees

* min_samples_split - minimum required number of samples to consider a split

<br>

##### **A demo**

A quick AdaBoost demonstration (a very simple example that is very likely not going to reflect real-life scenarios).

```{python}
import sys
import pandas as pd
import numpy as np

from rdkit import Chem

# Standardise molecules
from scikit_mol.standardizer import Standardizer
# Import fingerprints & descriptors
from scikit_mol.fingerprints import MorganFingerprintTransformer
from scikit_mol.descriptors import MolecularDescriptorTransformer
# Import smi2mol transformer
from scikit_mol.conversions import SmilesToMolTransformer

from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import AdaBoostClassifier
```

```{python}
# Show system info
print(sys.version)
```

```{python}
# Reading from the previously saved file
df_ache = pd.read_csv("chembl_d_ache_33.tsv", sep = ",")
print(df_ache.shape)
df_ache.head()
```

Some simple data explorations

```{python}
df_ache.value_counts("max_phase")
```

```{python}
df_ache[["pchembl_value"]].describe()
```

<br>

##### **Model building using pipeline**

Likely model forming steps:

*Another similar package, molpipeline - https://github.com/basf/MolPipeline - special feature is that it handles errors within the pipeline*

* SMILES sanitisation
    - ?using scikit-mol or Datamol (or own script)

* Define X, y

* Data splitting - randomly splits data 
```{python}
X_train, y_train, X_test, y_test = train_test_split()
```

* Create a pipeline using Scikit-learn

    - possibly chaining AdaBoostClassifier and HistGradientBoostingClassifier

    - using Scikit-mol transformers

```{python}
mlpipe = make_pipeline(

    # Convert SMILES to RDKit molecules
    SmilesToMolTransformer(), 

    # Molecule standardisations
    Standardizer(),

    # RDKit2D descriptors (or MorganFingerprintTransformer() - to be decided)
    MolecularDescriptorTransformer()
    )
```


    - Which data features to use for targets e.g. fingerprints (e.g. Morgan fingerprints best for larger dataset) or RDKit 2D descriptors (useful for smaller datasets) or others

    *Useful Jupyter notebook explaining when to best use parallel calculations to calculate molecular fingerprints and descriptors - https://github.com/EBjerrum/scikit-mol/blob/main/notebooks/07_parallel_transforms.ipynb*

    - AdaBoost classifier (target: max_phase)
    (likely not going to use regressor which uses pchembl_value, which is the -log of published activity values e.g. IC50 - [ChEMBL database link](https://chembl.gitbook.io/chembl-interface-documentation/frequently-asked-questions/chembl-data-questions#what-is-pchembl) that explains this)
