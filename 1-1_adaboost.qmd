---
title: "Boosted trees"
subtitle: "Series 2.3.1 - Adaptive boosting"
author: Jennifer HY Lin
date: '2024-4-17'
draft: true
categories: 
    - Machine learning projects
    - Tree models
    - Pandas
    - Scikit-learn
    - RDKit
    - ChEMBL database
    - Python
jupyter: python3
format: html
bibliography: references.bib
---

##### **Some introductions**

Adaptive Boost is also known as AdaBoost, which has originated from Robert E. Schapire in 1990 [@schapire1990], [@mlbook2022]. This concept was further introduced in 1996 by Robert Schapire and Yoav Freund at a conference and led to another publication [@freund1997].

As quoted from [scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#adaboost), this informs what an AdaBoost algorithm is doing:

> ...fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data.

Basically, this means that increased weights are given to incorrectly predicted samples at each iteration when running the AdaBoost models. Less weights are given to the correctly predicted ones. Overall, this forces the AdaBoost models to focus more on the less accurately predicted samples. In the end, the prediction from these iterations are combined to produce a final prediction via a weighted majority vote style (signature of tree models).

Overall, AdaBoost algorithm can be used for classification or regression problems. The main difference between bagging and boosting here is that boosting only uses random subsets of training samples drawn from the training set without any replacements.

Parameters to tune:

* n_estimators - number of weak learners

* learning_rate - contributions of weak learners in the final combination

* max_depth - depth of trees

* min_samples_split - minimum required number of samples to consider a split

<br>

##### **Some demo**

A quick demonstration or example of AdaBoost.

```{python}
import sys
import pandas as pd
import numpy as np

from sklearn.ensemble import AdaBoostRegressor
```

```{python}
# Show system info
print(sys.version)
```

```{python}
# Reading from the previously saved file
df_ache = pd.read_csv("chembl_d_ache_33.tsv", sep = ",")
```

```{python}
print(df_ache.shape)
df_ache.head()
```

Some simple data explorations

```{python}
df_ache.value_counts("max_phase")
```

```{python}
df_ache.value_counts("pchembl_value")
```

```{python}
df_ache[["pchembl_value"]].describe()
```

Decide what training data to use for targets e.g. fingerprints or RDKit 2D descriptors or others

AdaBoost regressor (target: pchembl_value) or classifier (target: max_phase)
