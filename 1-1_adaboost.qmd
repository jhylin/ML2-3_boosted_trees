---
title: "Boosted trees"
subtitle: "Series 2.3.1 - AdaBoost, ?Histogram-based gradient boosting and Scikit-mol"
author: Jennifer HY Lin
date: '2024-4-24'
draft: true
categories: 
    - Machine learning projects
    - Tree models
    - Pandas
    - Scikit-learn
    - RDKit
    - ChEMBL database
    - Python
jupyter: python3
format: html
bibliography: references.bib
---

##### **Quick overview**

AdaBoost
?Histogram-based gradient boosting
A sample demo using scikit_learn's AdaBoostClassifier & scikit_mol's transformers

##### **Some introductions**

###### **AdaBoost**

Adaptive Boost or AdaBoost has originated from Robert E. Schapire in 1990 [@schapire1990], [@mlbook2022], and was further introduced in 1996 by Robert Schapire and Yoav Freund at a conference which also led to a publication [@freund1997].

As quoted from [scikit-learn](https://scikit-learn.org/stable/modules/ensemble.html#adaboost), an AdaBoost algorithm is doing this:

> ...fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data.

Note: weak learners = an ensemble of very simple base classifiers e.g. decision tree stumps [@mlbook2022]

During the process of running the algorithm, increased weights are given to the incorrectly predicted samples at each iteration, and less weights are given to the correctly predicted ones. This then forces the AdaBoost models to focus more on the less accurately predicted samples with the aim to improve ensemble performance. The predictions from these iterations are combined to produce a final prediction via a weighted majority vote style, which is a well-known signature of tree models. Overall, AdaBoost algorithm can be used for classification or regression problems. The main difference between bagging and boosting is that boosting only uses random subsets of training samples drawn from the training dataset without any replacements [@mlbook2022]. One caveat to note is that AdaBoost tend to overfit training data (high variance).

Parameters to tune:

* n_estimators - number of weak learners

* learning_rate - contributions of weak learners in the final combination

* max_depth - depth of trees

* min_samples_split - minimum required number of samples to consider a split

###### **Histogram-based gradient boosting**

TBC

<br>

##### **A demo**

A quick AdaBoost demonstration (a very simple example that is very likely not going to reflect real-life scenarios).

```{python}
import sys
import pandas as pd
import numpy as np

from rdkit import Chem

# Import Scikit_mol
## Check and clean SMILES
from scikit_mol.utilities import CheckSmilesSanitazion
## Standardise molecules
from scikit_mol.standardizer import Standardizer
## Import fingerprints & descriptors
from scikit_mol.fingerprints import MorganFingerprintTransformer
from scikit_mol.descriptors import MolecularDescriptorTransformer
## Import smi2mol transformer
from scikit_mol.conversions import SmilesToMolTransformer

from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import AdaBoostClassifier
```

```{python}
# Show system info
print(sys.version)
```

```{python}
# Reading from the previously saved file
df_ache = pd.read_csv("chembl_d_ache_33.tsv", sep = ",")
print(df_ache.shape)
df_ache.head()
```

Some simple data explorations

```{python}
df_ache.value_counts("max_phase")
```

```{python}
df_ache[["pchembl_value"]].describe()
```

<br>

##### **Model building using pipeline**

Likely model forming steps:

*Another similar package, molpipeline - https://github.com/basf/MolPipeline - special feature is that it handles errors within the pipeline*

* Define X, y

* SMILES sanitisation

To avoid confusion, this post is likely to focus only on scikit_mol which has a different way to handle SMILES errors. Another useful way to deal with SMILES errors is molpipeline's SMILES error handling, with an example shown in one of its notebooks - https://github.com/basf/MolPipeline/blob/main/notebooks/03_error_handling.ipynb. The main difference from what I could see was that molpipeline takes into account all the invalid SMILES by giving each invalid SMILES a "nan" label in the pipeline process - this maintains the matrix shape and good for tracking down the problematic SMILES (compounds).

```{python}
checksmi = CheckSmilesSanitazion()

X_valid, y_valid, X_errors, y_errors = checksmi.sanitize(X, y)

# Likely using X_valid & y_valid for further work (but this also means removing the invalids or errors completely from the training dataset)
```

* Data splitting - randomly splits data 
```{python}
# X_train, y_train, X_test, y_test = train_test_split(X_valid, y_valid)
```

* Create a pipeline using Scikit-learn

    - possibly chaining AdaBoostClassifier and HistGradientBoostingClassifier
    - using Scikit-mol transformers

Links re. building pipeline in Scikit-learn:
- Pipeline module - https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn-pipeline-pipeline

- make_pipeline module (simpler pipeline construction, without naming estimators ourselves but rather naming them automatically) - https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn-pipeline-make-pipeline

```{python}
mlpipe = make_pipeline(
    # Convert SMILES to RDKit molecules
    SmilesToMolTransformer(), 
    # Molecule standardisations
    Standardizer(),
    # RDKit2D descriptors (or MorganFingerprintTransformer() - to be decided)
    MolecularDescriptorTransformer(),
    # Adaptive boost classifier
    # to change parameters: **param_adaboost = {max_depth = 4}
    # AdaBoostClassifier(**param_adaboost)
    AdaBoostClassifier()
    )
```


    - Which data features to use for targets e.g. fingerprints (e.g. Morgan fingerprints best for larger dataset) or RDKit 2D descriptors (useful for smaller datasets) or others

    *Useful Jupyter notebook explaining when to best use parallel calculations to calculate molecular fingerprints and descriptors - https://github.com/EBjerrum/scikit-mol/blob/main/notebooks/07_parallel_transforms.ipynb*

    - AdaBoost classifier (target: max_phase)
    (likely not going to use regressor which uses pchembl_value, which is the -log of published activity values e.g. IC50 - [ChEMBL database link](https://chembl.gitbook.io/chembl-interface-documentation/frequently-asked-questions/chembl-data-questions#what-is-pchembl) that explains this)


```{python}
# # apply the pipeline to training data
# mlpipe.fit(X_train, y_train)
# # use the pipeline to predict on testing data
# mlpipe.predict(y_test)
```


```{python}
# Calculate metrics of the model
```